{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named builtins",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-86c1b6243faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mespnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoadInputsAndTargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chaitanay/.local/lib/python2.7/site-packages/torch/__init__.pyc\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_utils_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_multiprocessing_environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring_classes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_string_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m __all__ = [\n",
      "\u001b[0;32m/home/chaitanay/.local/lib/python2.7/site-packages/torch/_six.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named builtins"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import numpy as np\n",
    "from espnet.utils.io_utils import LoadInputsAndTargets\n",
    "from espnet.utils.training.batchfy import make_batchset\n",
    "from espnet.asr.pytorch_backend.asr_init import load_trained_model\n",
    "from espnet.asr.pytorch_backend.asr import CustomConverter\n",
    "import cmudict\n",
    "import re\n",
    "import Hmax\n",
    "def getPsuedoKeyword(target_phone, already_present):\n",
    "    a = cmudict.dict()\n",
    "    b = cmudict.words()\n",
    "    found = False\n",
    "    for word in b:\n",
    "        for lst in a[word]:\n",
    "            for phone in lst:\n",
    "                if(target_phone == phone and already_present.get(word) == None):\n",
    "                    already_present[word] = 1\n",
    "                    return word\n",
    "                    \n",
    "                if(re.search(target_phone,phone) and len(target_phone) !=1 and already_present.get(word) == None):\n",
    "                    return word\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "with open(\"/home/ram/chaitanay/features/Great/deltafalse/data.json\", \"r\") as f:\n",
    "    train_json = json.load(f)[\"utts\"]\n",
    "\n",
    "# with open(\"/root/kws_data/dump/test/deltafalse/data.json\", \"r\") as f:\n",
    "#     test_json = json.load(f)[\"utts\"]\n",
    "\n",
    "# with open(\"/root/kws_data/dump/dev/deltafalse/data.json\", \"r\") as f:\n",
    "#     dev_json = json.load(f)[\"utts\"]\n",
    "\n",
    "train_data_batches = make_batchset(train_json, 1)\n",
    "# test_data_batches = make_batchset(test_json, 1)\n",
    "# dev_data_batches = make_batchset(dev_json, 1)\n",
    "\n",
    "load_tr = LoadInputsAndTargets(\n",
    "        mode='asr', load_output=True, preprocess_conf=None,\n",
    "        preprocess_args={'train': True}  # Switch the mode of preprocessing\n",
    ")\n",
    "\n",
    "converter = CustomConverter(subsampling_factor=1, dtype=torch.float32)\n",
    "\n",
    "model, train_args = load_trained_model(\"/home/ram/chaitanay/model/model.acc.best\")\n",
    "model = model.to(device=device)\n",
    "model.dec.sampling_probability = 1.0\n",
    "\n",
    "phone_to_int = dict(zip(train_args.char_list, np.arange(len(train_args.char_list))))\n",
    "keyword = \"G R EY T\"\n",
    "keyword_tokens = torch.tensor([[phone_to_int[phn] for phn in keyword.split(\" \")]]).to(device)\n",
    "\n",
    "encoder_output = 0\n",
    "\n",
    "# def get_att_score2(att_w_list):\n",
    "#     atts = [ele.detach().cpu().numpy().flatten() for ele in att_w_list]\n",
    "#     atts = np.array(atts)\n",
    "#     sum_att = np.prod(atts, axis=0)\n",
    "#     att_score = np.trapz(sum_att)\n",
    "#     return att_score\n",
    "\n",
    "# def get_att_score(att_w_list):\n",
    "#     atts = [ele.detach().cpu().numpy().flatten() for ele in att_w_list]\n",
    "#     atts = np.array(atts)\n",
    "#     init_att = atts[0]\n",
    "#     for att in atts[1:]:\n",
    "#         init_att = np.abs(init_att - att)\n",
    "#     att_score = np.mean(init_att)\n",
    "#     return att_score\n",
    "encoder_output = 0\n",
    "# with open(\"train_confidences_ctc_att_small_sp1_as.txt\", \"a\") as f:\n",
    "for row in train_data_batches:\n",
    "        data_input = [load_tr(row)]\n",
    "        data = converter(data_input, device)\n",
    "        encoder_output, hlens, _ = model.enc(data[0], data[1])\n",
    "        break\n",
    "        \n",
    "encoder_output = encoder_output.detach().cpu().numpy()\n",
    "\n",
    "        \n",
    "        \n",
    "Q_value = dict()\n",
    "column =0\n",
    "pickle_in = open(\"Confusion.Matrix\",\"rb\")\n",
    "Confusion_Matrix = pickle.load(pickle_in)\n",
    "pickle_in = open(\"Deletion.vector\",\"rb\")\n",
    "Deletion_vector = pickle.load(pickle_in)\n",
    "pickle_in = open(\"Character_dict\",\"rb\")\n",
    "char_dict = pickle.load(pickle_in)\n",
    "integer_dict = dict(map(reverse,char_dict.items()))\n",
    "\n",
    "for ph in keyword:\n",
    "    phone = ph.upper()\n",
    "    print(\"calculating for phone \",phone)\n",
    "    Q_x = []\n",
    "    number = 0\n",
    "    word_dict = dict()\n",
    "    for n in range(10):\n",
    "        class_Hm = Hmax()\n",
    "        psd_word = getPsuedoKeyword(phone,word_dict)\n",
    "        Hmax,op_list=class_Hm.getHmax(psd_word,Encoder_output,0,0,1.0,[])\n",
    "        for op in op_list:\n",
    "            oper = op[0]\n",
    "            src_pos = op[1]\n",
    "            dst_pos = op[2]\n",
    "            if(oper == 'replace' and psd_word[dst_pos] = phone ):\n",
    "                val = Encoder_output[char_dict[phone]][column]*Confusion_Matrix[char_dict[phone]][Hmax[dest_pos]]\n",
    "                Q_x.append(val)\n",
    "                number = number + 1\n",
    "            elif(oper == 'delete' and psd_word[dst_pos] = phone ):\n",
    "                val = Deletion_vector[phone]\n",
    "                Q_x.append(val)\n",
    "                number =number + 1\n",
    "    sm=0 \n",
    "    for val in Q_x:\n",
    "        sm = sm + val\n",
    "        \n",
    "    if(Q_value.get(phone) == None):\n",
    "        Q_value[phone] = sm/float(number)\n",
    "    else:\n",
    "        Q_value[phone] = (Q_value[phone] + sm/float(number))/2.0\n",
    "        \n",
    "\n",
    "print(Q_value)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #hs_pad is the output of the encoder\n",
    "#                 ctc_loss = model.ctc(hs_pad, hlens, keyword_tokens)\n",
    "#                 ctc_loss = float(ctc_loss.cpu().detach().numpy())\n",
    "#                 att_loss, acc, _, att_w_list = model.dec(hs_pad, hlens, keyword_tokens)\n",
    "#                 att_score = get_att_score(att_w_list)\n",
    "\n",
    "#                 att_loss = float(att_loss.cpu().detach().numpy())\n",
    "#                 f.write(str(ctc_loss) + \",\" + str(att_loss) + \",\" + str(acc) + \",\" + str(att_score) + \",\" + str(row[0][1][\"output\"][0][\"token\"]))\n",
    "#                 f.write(\"\\n\")\n",
    "                \n",
    "# with open(\"test_confidences_ctc_att_small_sp1_as.txt\", \"a\") as f:\n",
    "#         for row in test_data_batches:\n",
    "#                 data_input = [load_tr(row)]\n",
    "#                 data = converter(data_input, device)\n",
    "#                 hs_pad, hlens, _ = model.enc(data[0], data[1])\n",
    "#                 ctc_loss = model.ctc(hs_pad, hlens, keyword_tokens)\n",
    "#                 ctc_loss = float(ctc_loss.cpu().detach().numpy())\n",
    "#                 att_loss, acc, _, att_w_list = model.dec(hs_pad, hlens, keyword_tokens)\n",
    "#                 att_score = get_att_score(att_w_list)\n",
    "\n",
    "#                 att_loss = float(att_loss.cpu().detach().numpy())\n",
    "#                 f.write(str(ctc_loss) + \",\" + str(att_loss) + \",\" + str(acc) + \",\" + str(att_score) + \",\" + str(row[0][1][\"output\"][0][\"token\"]))\n",
    "#                 f.write(\"\\n\")\n",
    "\n",
    "# with open(\"dev_confidences_ctc_att_small_sp1_as.txt\", \"a\") as f:\n",
    "#         for row in dev_data_batches:\n",
    "#                 data_input = [load_tr(row)]\n",
    "#                 data = converter(data_input, device)\n",
    "#                 hs_pad, hlens, _ = model.enc(data[0], data[1])\n",
    "#                 ctc_loss = model.ctc(hs_pad, hlens, keyword_tokens)\n",
    "#                 ctc_loss = float(ctc_loss.cpu().detach().numpy())\n",
    "#                 att_loss, acc, _, att_w_list = model.dec(hs_pad, hlens, keyword_tokens)\n",
    "#                 att_score = get_att_score(att_w_list)\n",
    "\n",
    "#                 att_loss = float(att_loss.cpu().detach().numpy())\n",
    "#                 f.write(str(ctc_loss) + \",\" + str(att_loss) + \",\" + str(acc) + \",\" + str(att_score) + \",\" + str(row[0][1][\"output\"][0][\"token\"]))\n",
    "                \n",
    "                \n",
    "#encoder output is in encoder_output\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
